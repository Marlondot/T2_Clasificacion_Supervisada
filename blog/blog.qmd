---
title: "Creacion scorecard para calculo de puntaje crediticio a partir de un modelo predictivo"
bibliography: references.bib
author:
  - name: Daniel Daza Macias
    email: dadazam@unal.edu.co
  - name: Daniel Santiago Cadavid Montoya
    email: dcadavid@unal.edu.co
  - name: Jose Daniel Bustamante Arango
    email: jobustamantea@unal.edu.co
  - name: Marlon Calle Areiza
    email: mcallea@unal.edu.co
  - name: Ronald Gabriel Palencia
    email: ropalencia@unal.edu.co
format:
  html:
    code-fold: true
jupyter: python3
echo: false
theme:
          light: flatly
          dark: darkly
toc: true
appendix-style: default
---

## Introducción

El siguiente trabajo tiene como objetivo la realización de una scorecard que permita a las personas calcular su puntaje crediticio de manera sencilla.

Para realizar el siguiente reporte utilizamos la base de datos proporcionada por el docente del curso de Técnicas en Aprendizaje Estadístico de la Universidad Nacional de Colombia.

## Importe y análisis de datos

Para comenzar se hará un cargue del conjunto de datos:

```{python}
#| tbl-cap: Datos iniciales
#| label: tbl-import-presentacion-inicial


import numpy as np
import pandas as pd
import seaborn as sb
from tabulate import tabulate 
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, ward
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc
from sklearn.feature_selection import f_classif
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.stats import chi2_contingency

pd.set_option("display.max_columns", 4)
#TODO: Explicar valores Nan
#TODO: Revisar el tema de la low_memory=False
datos_raw = pd.read_csv('loan_data_2007_2014.csv', na_values='PrivacySuppressed',low_memory=False)

datos_raw.head(5)


```

Como se puede ver en @tbl-import-presentacion-inicial la base de datos contiene 74 columnas además de 466285 observaciones.

Por otra parte nuestra variable de interés será loan_status o estado del prestamo de cada observación que tiene entre posibles valores: 'Fully Paid', 'Charged Off', 'Current', 'Default', 'Late (31-120 days)','In Grace Period', 'Late (16-30 days)', 'Does not meet the credit policy. Status:Fully Paid' y 'Does not meet the credit policy. Status:Charged Off'. Así y como nuestro objetivo parte de la clasificación en dos grupos partiremos de esta variable para otorgar a cada observación el valor de: incumplió con obligaciones financieras (0 en la nueva columna creada 'Obligaciones_financieras') o cumplió con las obligaciones financieras(1 en la nueva columna creada 'Obligaciones_financieras'), el primero siendo equivalente a los valores 'Charged Off', 'Default', 'Late (31-120 days)' y 'Does not meet the credit policy. Status:Charged Off' en 'loan_status'.

```{python}

pd.set_option("display.max_columns", 15)#Setting max number of columns


df=datos_raw

df['Obligaciones_financieras'] = np.where(df.loc[:, 'loan_status'].isin(['Charged Off', 'Default', 'Late (31-120 days)','Does not meet the credit policy. Status:Charged Off']), 0, 1)

```


### Selección y transformación de variables

Para comenzar a seleccionar variables y como nuestro interés de acuerdo al objetivo es tener la mayor cantidad de datos sin imputar obviaremos para el ajuste del modelo aquellas cuya cantidad de datos nulos sea mayor al 70%, dichas variables se pueden ver en @tbl-variables-nulos

```{python}
#| tbl-cap: Variables con porcentaje de nulo mayor al 30%
#| label: tbl-variables-nulos

from IPython.display import Markdown
from tabulate import tabulate

missing_values = df.isnull().mean()
missing_values = missing_values[missing_values>0.7]


table = [[i, j] for i,j in missing_values.items()]

Markdown(tabulate(
  table, 
  headers=["Variable","Porcentaje de datos nulos"]
))

```


Además de esto eliminaremos las variables: 'id', 'member_id', 'sub_grade', 'emp_title', 'url', 'title', 'zip_code', 'next_pymnt_d', 'recoveries', 'collection_recovery_fee', 'total_rec_prncp', 'total_rec_late_fee' y 'policy_code', que consideramos no aportan mucha información a la hora de definir las obligaciones económicas de una persona.

Procederemos a realizar una matriz de correlación de Pearson para ver qué variables podrían ser explicadas por otras y obviarlas:

```{python}



l = ['id', 'member_id','sub_grade','emp_title','url','title','zip_code','next_pymnt_d','recoveries','collection_recovery_fee','total_rec_prncp','total_rec_late_fee','policy_code']
df.drop(list(missing_values.index),inplace=True, axis=1)
df.drop(l ,inplace=True, axis=1)

```


![Matriz de correlacion](fig_1_corr_pearson.png){#fig-matriz-corr-1}

Así y de acuerdo @fig-matriz-corr-1 se eliminarán aquellas columnas que posean un correlación de al menos 60% es decir las variables: 'total_rec_int', 'total_pymnt_inv', 'total_pymnt', 'installment', 'funded_amnt', 'funded_amnt_inv', 'out_prncp_inv',
'total_acc' y 'total_rev_hi_lim'.

Al final resultaríamos con las variables encontradas en @tbl-variables-resultantes, donde aquellas de tipo object son las que reconoceremos como categóricas para posterior análisis y transformación:

```{python}
#| tbl-cap: Variables resultantes y tipos
#| label: tbl-variables-resultantes

#Sacamos las variables continuas 
variables_correlacionadas = ['total_rec_int','total_pymnt_inv','total_pymnt','installment','funded_amnt', 'funded_amnt_inv','out_prncp_inv','total_acc','total_rev_hi_lim']
df.drop(variables_correlacionadas ,inplace=True, axis=1)


table = [[i, j] for i,j in df.dtypes.items()]

Markdown(tabulate(
  table, 
  headers=["Variable","Porcentaje de datos nulos"]
))

```


#### Transformación de variables categóricas

Para comenzar cambiaremos los valores categóricos de la variable emp_length por la cantidad de años como se puede observar en @tbl-cam-var-1.

```{python}
#| tbl-cap: Cambio en variable emp_length
#| label: tbl-cam-var-1

cambio = {
    '< 1 year':0,
    '1 year': 1,
    '2 years': 2,
    '3 years': 3,
    '4 years': 4,
    '5 years': 5,
    '6 years': 6,
    '7 years': 7,
    '8 years': 8,
    '9 years': 9,
    '10 years': 10,
    '10+ years':10
}



table = [[i, cambio[i], j] for i,j in df.emp_length.value_counts().items()]

Markdown(tabulate(
  table, 
  headers=["Valor", "Nuevo valor","Cantidad de datos"]
))
```

Continuando, se pasará a eliminar la columna 'pymnt_plan' ya que solo tiene el 0.0019% de valores en yes frente al resto de no como se puede ver en @tbl-cam-var-2 por lo que no la consideraremos como muy aportante al modelo.


```{python}
#| tbl-cap: Eliminación variable pymnt_plan
#| label: tbl-cam-var-2

df['emp_length'] = df['emp_length'].map(cambio)


table = [[i, j] for i,j in df.pymnt_plan.value_counts().items()]

Markdown(tabulate(
  table, 
  headers=["Valor","Cantidad de datos"]
))
```

Además, se eliminarán las variables 'last_pymnt_d', 'last_credit_pull_d', 'earliest_cr_line', 'issue_d', 'addr_state', 'initial_list_status' y 'mths_since_last_delinq', que contienen fechas y no se consideran suficiente aportantes para el modelo.

Para terminar se eliminarán también las columnas 'application_type', pues sólo existe un único valor que comparten todas las observaciones y 'loan_status', que es la variable de la que partimos para establecer nuestra variable objetivo.


## Selección de variables a partir de WoE y Valor de Información (IV)

Como se puede ver en @tbl-cantidad-nulos-final donde se encuentran las variables resultantes y la cantidad de nulos de cada uno debemos hacer una imputación, dicha imputación consistirá en reemplazar valores nulos por la moda, en el caso de las variables categóricas, y por la mediana en el caso de las continuas.

Dicha imputación se planteará en una función pero sólo se aplicará debidamente luego de dividir el conjunto de datos en datos de entrenamiento y datos de testeo.

```{python}
#| tbl-cap: Cantidad de nulos en variables resultatnes
#| label: tbl-cantidad-nulos-final


df.drop('loan_status', axis=1, inplace=True)
df.drop('pymnt_plan', axis=1, inplace=True)
df.drop('application_type', axis=1, inplace=True)
df.drop(['last_pymnt_d', 'last_credit_pull_d','earliest_cr_line','issue_d','addr_state','initial_list_status','mths_since_last_delinq'], axis=1, inplace=True)




table = [[i, j] for i,j in df.isna().sum().items()]

Markdown(tabulate(
  table, 
  headers=["Variable","Cantidad de nulos"]
))

```

A continuación aplicaremos la división de datos de 80% y 20% para entrenamiento y testeo respectivamente:

```{python}
#| tbl-cap: Vista inicial a datos de entrenamiento
#| label: tbl-x-train-inicial


# split data into 70/30 while keeping the distribution of bad loans in test set same as that in the pre-split dataset
from sklearn.model_selection import train_test_split
X = df.drop('Obligaciones_financieras', axis = 1)
y = df['Obligaciones_financieras']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)

# hard copy the X datasets to avoid Pandas' SetttingWithCopyWarning when we play around with this data later on.
# this is currently an open issue between Pandas and Scikit-Learn teams
X_train, X_test = X_train.copy(), X_test.copy()

X_train
```
```{python}
#| tbl-cap: Vista inicial a datos de testeo
#| label: tbl-x-test-inicial


X_test
```

Por lo que terminaremos con 373028 datos de entrenamiento y 93257 datos de testeo como se puede ver en @tbl-x-train-inicial y @tbl-x-test-inicial.

Así imputaremos cada conjunto y pasaremos a hacer selección de categorías con WoE y valor de información (IV) calculado donde seleccionaremos sólo aquellas que tengan valores por encima de 0.02 para este último.



```{python}

# We define a function to calculate WoE of continuous variables. This is same as the function we defined earlier for discrete variables.
# The only difference are the 2 commented lines of code in the function that results in the df being sorted by continuous variable values
def woe_ordered_continuous(df, continuous_variabe_name, y_df):
    df = pd.concat([df[continuous_variabe_name], y_df], axis = 1)
    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
    df = df.iloc[:, [0, 1, 3]]
    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()
    df['n_good'] = df['prop_good'] * df['n_obs']
    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()
    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()
    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
    #df = df.sort_values(['WoE'])
    #df = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE'] = df['WoE'].diff().abs()
    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV'] = df['IV'].sum()
    return df

# The function takes 3 arguments: a dataframe (X_train_prepr), a string (column name), and a dataframe (y_train_prepr).
# The function returns a dataframe as a result.
def woe_discrete(df, cat_variabe_name, y_df):
      df = pd.concat([df[cat_variabe_name], y_df], axis = 1)
      df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                      df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
      df = df.iloc[:, [0, 1, 3]]
      df.columns = [df.columns.values[0], 'n_obs', 'prop_good']
      df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()
      df['n_good'] = df['prop_good'] * df['n_obs']
      df['n_bad'] = (1 - df['prop_good']) * df['n_obs']
      df['prop_n_good'] = df['n_good'] / df['n_good'].sum()
      df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()
      df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
      df = df.sort_values(['WoE'])
      df = df.reset_index(drop = True)
      df['diff_prop_good'] = df['prop_good'].diff().abs()
      df['diff_WoE'] = df['WoE'].diff().abs()
      df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
      df['IV'] = df['IV'].sum()
      return df
```


### Análisis de IV para variables categóricas

Comenzaremos analizando la variable  home_ownership que como se puede notar en @tbl-iv-home_ownership tiene un IV mayor de 0.02, por lo que la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de home_ownership
#| label: tbl-iv-home_ownership


# Create copies of the 4 training sets to be preprocessed using WoE
X_train_prepr = X_train.copy()
y_train_prepr = y_train.copy()
X_test_prepr = X_test.copy()
y_test_prepr = y_test.copy()

resultados_cat_iv=dict()


to_prt=woe_discrete(X_train_prepr, 'home_ownership', y_train_prepr)
resultados_cat_iv["home_ownership"]=to_prt["IV"][0]
to_prt
```


Continuaremos analizando la variable  verification_status que como se puede notar en @tbl-iv-verification_status tiene un IV mayor de 0.02, por lo que la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de verification_status
#| label: tbl-iv-verification_status



to_prt=woe_discrete(X_train_prepr, 'verification_status', y_train_prepr)
resultados_cat_iv["verification_status"]=to_prt["IV"][0]
to_prt
```

Así analizando la variable  purpose que como se puede notar en @tbl-iv-purpose tiene un IV mayor de 0.02, por lo que la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de purpose
#| label: tbl-iv-purpose



to_prt=woe_discrete(X_train_prepr, 'purpose', y_train_prepr)
resultados_cat_iv["purpose"]=to_prt["IV"][0]
to_prt
```

Así analizando la variable grade que como se puede notar en @tbl-iv-grade tiene un IV mayor de 0.02, por lo que la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de grade
#| label: tbl-iv-grade



to_prt=woe_discrete(X_train_prepr, 'grade', y_train_prepr)
resultados_cat_iv["grade"]=to_prt["IV"][0]
to_prt
```

Así analizando la variable term que como se puede notar en @tbl-iv-term tiene un IV mayor de 0.02, por lo que la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de term
#| label: tbl-iv-term



to_prt=woe_discrete(X_train_prepr, 'term', y_train_prepr)
resultados_cat_iv["term"]=to_prt["IV"][0]
to_prt
```

Por lo que concluimos seleccionando todas las variables categóricas como se puede ver en el resumen de @tbl-iv-cat-res.

```{python}
#| tbl-cap: IV de variables categóricas
#| label: tbl-iv-cat-res

table = [[i, j] for i,j in resultados_cat_iv.items()]

Markdown(tabulate(
  table, 
  headers=["Variable","IV"]
))

```
### Análisis de IV para numéricas

Para comenzar se hará un análisis rápido del IV de cada variable teniendo en cuenta todas las observaciones del X_train, dividiendo variables continuas en 20 intervalos y no continuas en cantidad de valores únicos:

```{python}
#| tbl-cap: Resumen de IV para variables continuas
#| label: tbl-iv-res-cont


res_to_print={"loan_amnt":	0.00568593,
"int_rate":	0.334257,
"emp_length":	0.0046393,
"annual_inc":	"inf",
"dti":	0.0266219,
"delinq_2yrs":	"inf",
"inq_last_6mths":	"inf",
"open_acc":	"inf",
"pub_rec":	"inf",
"revol_bal":	"inf",
"revol_util":	"inf",
"out_prncp":	0.645187,
"last_pymnt_amnt":	"inf",
"collections_12_mths_ex_med":	"inf",
"acc_now_delinq":	"inf",
"tot_coll_amt":	"inf",
"tot_cur_bal":	"inf"}


# resultados_cont_iv=dict()
# continuous_variabe_name=['loan_amnt', 'int_rate', 
#        'emp_length', 'annual_inc', 
#        'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc',
#        'pub_rec', 'revol_bal', 'revol_util', 'out_prncp', 'last_pymnt_amnt',
#        'collections_12_mths_ex_med', 'acc_now_delinq',
#        'tot_coll_amt', 'tot_cur_bal']

# no_se_cortan = set(['emp_length', 'delinq_2yrs', 'inq_last_6mths', 'pub_rec', 'collections_12_mths_ex_med', 'acc_now_delinq'])

# for continua in continuous_variabe_name:
#   if continua in no_se_cortan:
#     resultados_cont_iv[continua] = woe_ordered_continuous(X_train_prepr, continua, y_train_prepr)["IV"][0]
#   else:
#     nombre = continua+"_factor"
#     X_train_prepr[nombre] = pd.cut(X_train_prepr[continua], 20)
#     resultados_cont_iv[continua] = woe_ordered_continuous(X_train_prepr, nombre, y_train_prepr)["IV"][0]


table = [[i, j] for i,j in res_to_print.items()]

Markdown(tabulate(
  table, 
  headers=["Variable","IV"]
))


```

Como se puede observar en @tbl-iv-res-cont hay algunas variables cuyo valor de información es inf, esto se debe a la cantidad de observaciones positivas o negativas en algunos de los intervalos. Para controlar esto haremos el análisis de aquellas variables cuidando de posibles outliers que dificulten dicha división en intervalos

Así comenzaremos analizando la variable annual_inc que como se puede notar en @tbl-iv-annual_inc tiene un IV mayor de 0.02 considerando valores menores o iguales a 150000, por lo que la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de annual_inc
#| label: tbl-iv-annual_inc

ensayitos=X_train_prepr.copy()
ensayitos_temp = ensayitos[ensayitos['annual_inc']<=150000].copy()
ensayitos_temp['annual_inc_factor'] = pd.cut(ensayitos_temp['annual_inc'], 20)
woe_ordered_continuous(ensayitos_temp, 'annual_inc_factor',  y_train_prepr[ensayitos_temp.index])

```


Continuaremos con la variable delinq_2yrs que como se puede notar en @tbl-iv-delinq_2yrs tiene un IV menor a 0.02 considerando valores menores o iguales a 10, por lo que no la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de delinq_2yrs
#| label: tbl-iv-delinq_2yrs

ensayitos_temp = ensayitos[ensayitos['delinq_2yrs']<=10].copy()
woe_ordered_continuous(ensayitos_temp, 'delinq_2yrs',  y_train_prepr[ensayitos_temp.index])

```


Continuaremos con la variable inq_last_6mths que como se puede notar en @tbl-iv-inq_last_6mths tiene un IV mayor a 0.02 considerando valores menores o iguales a 5, por lo que la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de inq_last_6mths
#| label: tbl-iv-inq_last_6mths

ensayitos_temp = ensayitos[ensayitos['inq_last_6mths']<=8].copy()
woe_ordered_continuous(ensayitos_temp, 'inq_last_6mths',  y_train_prepr[ensayitos_temp.index])

```

Continuaremos con la variable open_acc que como se puede notar en @tbl-iv-open_acc tiene un IV menor a 0.02 considerando valores menores o iguales a 28, por lo que no la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de open_acc
#| label: tbl-iv-open_acc

ensayitos_temp = ensayitos[ensayitos['open_acc']<=28].copy()
ensayitos_temp['open_acc_factor'] = pd.cut(ensayitos_temp['open_acc'], 20)
#ensayitos['open_acc_factor'] = pd.cut(ensayitos['open_acc'], 3)
#woe_ordered_continuous(ensayitos, 'open_acc_factor', y_train_prepr)
woe_ordered_continuous(ensayitos_temp, 'open_acc_factor',  y_train_prepr[ensayitos_temp.index])

```

Continuaremos con la variable pub_rec que como se puede notar en @tbl-iv-pub_rec tiene un IV menor a 0.02 considerando valores menores o iguales a 0.9, por lo que no la mantendremos para el ajuste del modelo pues además, como se puede notar en la misma tabla, es complicado realizar una división en intervalos suficiente para que dicha variable entregue un IV aceptable.

```{python}
#| tbl-cap: IV de pub_rec
#| label: tbl-iv-pub_rec

ensayitos_temp = ensayitos[ensayitos['pub_rec']<=0.9].copy()
ensayitos_temp['pub_rec_factor'] = pd.cut(ensayitos_temp['pub_rec'], 20)
#ensayitos['pub_rec_factor'] = pd.cut(ensayitos['pub_rec'], 20)
woe_ordered_continuous(ensayitos_temp, 'pub_rec_factor',  y_train_prepr[ensayitos_temp.index])

```

Continuaremos con la variable revol_bal que como se puede notar en @tbl-iv-revol_bal tiene un IV menor a 0.02 considerando valores menores o iguales a 60000, por lo que no la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de revol_bal
#| label: tbl-iv-revol_bal

#ensayitos['revol_bal_factor'] = pd.cut(ensayitos['revol_bal'], 5)
#woe_ordered_continuous(ensayitos, 'revol_bal_factor', y_train_prepr)
ensayitos_temp = ensayitos[ensayitos['revol_bal']<=60000].copy()
ensayitos_temp['revol_bal_factor'] = pd.cut(ensayitos_temp['revol_bal'], 20)
woe_ordered_continuous(ensayitos_temp, 'revol_bal_factor',  y_train_prepr[ensayitos_temp.index])
```


Continuaremos con la variable revol_util que como se puede notar en @tbl-iv-revol_util tiene un IV mayor a 0.02 considerando valores menores o iguales a 105, por lo que la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de revol_util
#| label: tbl-iv-revol_util

#ensayitos['revol_util_factor'] = pd.cut(ensayitos['revol_util'], 20)
#woe_ordered_continuous(ensayitos, 'revol_util_factor', y_train_prepr)

ensayitos_temp = ensayitos[ensayitos['revol_util']<=105].copy()
ensayitos_temp['revol_util_factor'] = pd.cut(ensayitos_temp['revol_util'], 20)
woe_ordered_continuous(ensayitos_temp, 'revol_util_factor',  y_train_prepr[ensayitos_temp.index])
```

Continuaremos con la variable last_pymnt_amnt que como se puede notar en @tbl-iv-last_pymnt_amnt	tiene un IV en infinito incluso luego de haber manejado algunos outliers, por lo que no la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de last_pymnt_amnt
#| label: tbl-iv-last_pymnt_amnt

#ensayitos['revol_util_factor'] = pd.cut(ensayitos['revol_util'], 20)
#woe_ordered_continuous(ensayitos, 'revol_util_factor', y_train_prepr)
ensayitos_temp = ensayitos[ensayitos['last_pymnt_amnt']<=12000].copy()
ensayitos_temp['last_pymnt_amnt_factor'] = pd.cut(ensayitos_temp['last_pymnt_amnt'], 20)
woe_ordered_continuous(ensayitos_temp, 'last_pymnt_amnt_factor',  y_train_prepr[ensayitos_temp.index])
```

Continuaremos con la variable collections_12_mths_ex_med que como se puede notar en @tbl-iv-collections_12_mths_ex_med	tiene un IV menor a 0.02 considerando valores menores o iguales a 2, por lo que no la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de collections_12_mths_ex_med
#| label: tbl-iv-collections_12_mths_ex_med

#ensayitos['revol_util_factor'] = pd.cut(ensayitos['revol_util'], 20)
#woe_ordered_continuous(ensayitos, 'revol_util_factor', y_train_prepr)

ensayitos_temp = ensayitos[ensayitos['collections_12_mths_ex_med']<=2].copy()
woe_ordered_continuous(ensayitos_temp, 'collections_12_mths_ex_med',  y_train_prepr[ensayitos_temp.index])
```


Continuaremos con la variable acc_now_delinq que como se puede notar en @tbl-iv-acc_now_delinq	tiene un IV menor a 0.02 considerando valores menores o iguales a 0, por lo que no la mantendremos para el ajuste del modelo pues además, como se puede notar en la misma tabla, es complicado realizar una división en intervalos suficiente para que dicha variable entregue un IV aceptable.

```{python}
#| tbl-cap: IV de acc_now_delinq
#| label: tbl-iv-acc_now_delinq

#ensayitos['revol_util_factor'] = pd.cut(ensayitos['revol_util'], 20)
#woe_ordered_continuous(ensayitos, 'revol_util_factor', y_train_prepr)

ensayitos_temp = ensayitos[ensayitos['acc_now_delinq'] <= 0].copy()
ensayitos_temp['acc_now_delinq_factor'] = pd.cut(ensayitos_temp['acc_now_delinq'], 20)
woe_ordered_continuous(ensayitos_temp, 'acc_now_delinq_factor', y_train_prepr[ensayitos_temp.index])
```
Continuaremos con la variable tot_coll_amt que como se puede notar en @tbl-iv-tot_coll_amt	tiene un IV menor a 0.02 considerando valores menores o iguales a 750, por lo que no la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de tot_coll_amt
#| label: tbl-iv-tot_coll_amt


ensayitos_temp = ensayitos[ensayitos['tot_coll_amt']<=750].copy()
ensayitos_temp['tot_coll_amt_factor'] = pd.cut(ensayitos_temp['tot_coll_amt'], 20)
woe_ordered_continuous(ensayitos_temp, 'tot_coll_amt_factor',  y_train_prepr[ensayitos_temp.index])
```
Continuaremos con la variable tot_cur_bal que como se puede notar en @tbl-iv-tot_cur_bal	tiene un IV mayor a 0.02 considerando valores menores o iguales a 500000, por lo que la mantendremos para el ajuste del modelo.

```{python}
#| tbl-cap: IV de tot_cur_bal
#| label: tbl-iv-tot_cur_bal


ensayitos_temp = ensayitos[ensayitos['tot_cur_bal']<=500000].copy()
ensayitos_temp['tot_cur_bal_factor'] = pd.cut(ensayitos_temp['tot_cur_bal'], 25)
woe_ordered_continuous(ensayitos_temp, 'tot_cur_bal_factor', y_train_prepr[ensayitos_temp.index])
```

Así terminamos con un total de 12 variables categóricas y numéricas:

*   'purpose'
*   'verification_status'
*   'home_ownership'
*   'tot_cur_bal'
*   'dti'
*   'inq_last_6mths'
*   'out_prncp'
*   'int_rate'
*   'term'
*   'annual_inc'
*   'revol_util'
*   'grade'

## Ajuste del modelo y realización de Scorecard

Para continuar con el ajuste del modelo ya con las variables escogidas se hará una pipeline que tratará el conjunto de datos de diferentes maneras: Para las variables continuas se crearán variables dummies de intervalos, el último de ellos tendiendo a infinito, y se transformará el valor de la variable de cada observación en 1 y 0's de acuerdo al intervalo en el que caiga dicho valor. Para las variables categórica simplemente se crearán variabes dummies y se procederá como un OneHotEncoding normal. Esto se puede observar en @tbl-pre-datos-1.


```{python}
#| tbl-cap: Preprocesamiento total de datos
#| label: tbl-pre-datos-1


#Imputación de los datos
def imputar(X):    
    #Finding the median of the column having NaN
    mean_valuegrade=X['grade'].mode()
    mean_value1=X['emp_length'].median()
    mean_value2=X['annual_inc'].median()
    mean_value3=X['delinq_2yrs'].median()
    mean_value4=X['inq_last_6mths'].median()
    mean_value5=X['open_acc'].median()
    mean_value6=X['pub_rec'].median()
    mean_value7=X['revol_util'].median()
    mean_value8=X['collections_12_mths_ex_med'].median()
    mean_value9=X['acc_now_delinq'].median()
    mean_value10=X['tot_coll_amt'].median()
    mean_value11=X['tot_cur_bal'].median()
    
    # median of values in the same column
    X['grade'].fillna(value=mean_valuegrade, inplace=True)
    X['emp_length'].fillna(value=mean_value1, inplace=True)
    X['annual_inc'].fillna(value=mean_value2, inplace=True)
    X['delinq_2yrs'].fillna(value=mean_value3, inplace=True)
    X['inq_last_6mths'].fillna(value=mean_value4, inplace=True)
    X['open_acc'].fillna(value=mean_value5, inplace=True)
    X['pub_rec'].fillna(value=mean_value6, inplace=True)
    X['revol_util'].fillna(value=mean_value7, inplace=True)
    X['collections_12_mths_ex_med'].fillna(value=mean_value8, inplace=True)
    X['acc_now_delinq'].fillna(value=mean_value9, inplace=True)
    X['tot_coll_amt'].fillna(value=mean_value10, inplace=True)
    X['tot_cur_bal'].fillna(value=mean_value11, inplace=True)
  
imputar(X_train)
imputar(X_test)


# function to create dummy variables
def dummy_creation(df, columns_list):
    df_dummies = []
    for col in columns_list:
        df_dummies.append(pd.get_dummies(df[col], prefix = col, prefix_sep = ':'))
    df_dummies = pd.concat(df_dummies, axis = 1)
    df = pd.concat([df, df_dummies], axis = 1)
    return df

# apply to our final four categorical variables
X_train = dummy_creation(X_train, ['grade', 'home_ownership', 'verification_status', 'purpose'])
X_test = dummy_creation(X_test, ['grade', 'home_ownership', 'verification_status', 'purpose'])


ref_categories = ['out_prncp:>15,437', 'revol_util:>1.0', 'inq_last_6mths:>4', 'dti:>35.191', 
                  'annual_inc:>150K', 'int_rate:>20.281', 'term:60', 'purpose:major_purchcarhome_impr', 'verification_status:Not Verified', 
                  'home_ownership:MORTGAGE', 'grade:G', 'tot_cur_bal:0-100000']


# This custom class will create new categorical dummy features based on the cut-off points that we manually identified
# based on the WoE plots and IV above.
# Given the way it is structured, this class also allows a fit_transform method to be implemented on it, thereby allowing 
# us to use it as part of a scikit-learn Pipeline 
class WoE_Binning(BaseEstimator, TransformerMixin):
    def __init__(self, X): # no *args or *kargs
        self.X = X
    def fit(self, X, y = None):
        return self #nothing else to do
    def transform(self, X):
        X_new = X.loc[:, 'grade:A': 'grade:G']
        X_new['home_ownership:OWN'] = X.loc[:,'home_ownership:OWN']
        X_new['home_ownership:MORTGAGE'] = X.loc[:,'home_ownership:MORTGAGE']
        X_new['home_ownership:OTHER_NONE_RENT'] = sum([X['home_ownership:OTHER'], X['home_ownership:NONE'], X['home_ownership:RENT']])
        X_new = pd.concat([X_new, X.loc[:, 'verification_status:Not Verified':'verification_status:Verified']], axis = 1)

        # For the purpose of this column, we keep debt_consolidation (due to volume) and credit_card (due to unique characteristics) as separate cateogories
        # These categories have very few observations: educational, renewable_energy, vacation, house, wedding, car
        # car is the least risky so we will combine it with the other 2 least risky categories: home_improvement and major_purchase
        # educational, renewable_energy (both low observations) will be combined with small_business and moving
        # vacation, house and wedding (remaining 3 with low observations) will be combined with medical and other
        X_new['purpose:debt_consolidation'] = X.loc[:,'purpose:debt_consolidation']
        X_new['purpose:credit_card'] = X.loc[:,'purpose:credit_card']
        X_new['purpose:major_purch__car__home_impr'] = sum([X['purpose:major_purchase'], X['purpose:car'], X['purpose:home_improvement']])
        X_new['purpose:educ__ren_en__sm_b__mov'] = sum([X['purpose:educational'], X['purpose:renewable_energy'], X['purpose:small_business'], 
                                                        X['purpose:moving']])
        X_new['purpose:vacation__house__wedding__med__oth'] = sum([X['purpose:vacation'], X['purpose:house'], X['purpose:wedding'], 
                                                                   X['purpose:medical'], X['purpose:other']])
        X_new['term:36'] = np.where((X['term'] == 36), 1, 0)
        X_new['term:60'] = np.where((X['term'] == 60), 1, 0)
        X_new['int_rate:<7.071'] = np.where((X['int_rate'] <= 7.071), 1, 0)
        X_new['int_rate:7.071-10.374'] = np.where((X['int_rate'] > 7.071) & (X['int_rate'] <= 10.374), 1, 0)
        X_new['int_rate:10.374-13.676'] = np.where((X['int_rate'] > 10.374) & (X['int_rate'] <= 13.676), 1, 0)
        X_new['int_rate:13.676-15.74'] = np.where((X['int_rate'] > 13.676) & (X['int_rate'] <= 15.74), 1, 0)
        X_new['int_rate:15.74-20.281'] = np.where((X['int_rate'] > 15.74) & (X['int_rate'] <= 20.281), 1, 0)
        X_new['int_rate:>20.281'] = np.where((X['int_rate'] > 20.281), 1, 0)
        X_new['annual_inc:missing'] = np.where(X['annual_inc'].isnull(), 1, 0)
        X_new['annual_inc:<28,555'] = np.where((X['annual_inc'] <= 28555), 1, 0)
        X_new['annual_inc:28,555-37,440'] = np.where((X['annual_inc'] > 28555) & (X['annual_inc'] <= 37440), 1, 0)
        X_new['annual_inc:37,440-61,137'] = np.where((X['annual_inc'] > 37440) & (X['annual_inc'] <= 61137), 1, 0)
        X_new['annual_inc:61,137-81,872'] = np.where((X['annual_inc'] > 61137) & (X['annual_inc'] <= 81872), 1, 0)
        X_new['annual_inc:81,872-102,606'] = np.where((X['annual_inc'] > 81872) & (X['annual_inc'] <= 102606), 1, 0)
        X_new['annual_inc:102,606-120,379'] = np.where((X['annual_inc'] > 102606) & (X['annual_inc'] <= 120379), 1, 0)
        X_new['annual_inc:120,379-150,000'] = np.where((X['annual_inc'] > 120379) & (X['annual_inc'] <= 150000), 1, 0)
        X_new['annual_inc:>150K'] = np.where((X['annual_inc'] > 150000), 1, 0)
        X_new['dti:<=1.6'] = np.where((X['dti'] <= 1.6), 1, 0)
        X_new['dti:1.6-5.599'] = np.where((X['dti'] > 1.6) & (X['dti'] <= 5.599), 1, 0)
        X_new['dti:5.599-10.397'] = np.where((X['dti'] > 5.599) & (X['dti'] <= 10.397), 1, 0)
        X_new['dti:10.397-15.196'] = np.where((X['dti'] > 10.397) & (X['dti'] <= 15.196), 1, 0)
        X_new['dti:15.196-19.195'] = np.where((X['dti'] > 15.196) & (X['dti'] <= 19.195), 1, 0)
        X_new['dti:19.195-24.794'] = np.where((X['dti'] > 19.195) & (X['dti'] <= 24.794), 1, 0)
        X_new['dti:24.794-35.191'] = np.where((X['dti'] > 24.794) & (X['dti'] <= 35.191), 1, 0)
        X_new['dti:>35.191'] = np.where((X['dti'] > 35.191), 1, 0)
        X_new['inq_last_6mths:missing'] = np.where(X['inq_last_6mths'].isnull(), 1, 0)
        X_new['inq_last_6mths:0'] = np.where((X['inq_last_6mths'] == 0), 1, 0)
        X_new['inq_last_6mths:1-2'] = np.where((X['inq_last_6mths'] >= 1) & (X['inq_last_6mths'] <= 2), 1, 0)
        X_new['inq_last_6mths:3-4'] = np.where((X['inq_last_6mths'] >= 3) & (X['inq_last_6mths'] <= 4), 1, 0)
        X_new['inq_last_6mths:>4'] = np.where((X['inq_last_6mths'] > 4), 1, 0)
        # We will discretize on the deciles for revol_util
        X_new['revol_util:missing'] = np.where(X['revol_util'].isnull(), 1, 0)
        X_new['revol_util:<0.1'] = np.where((X['revol_util'] <= 0.1), 1, 0)
        X_new['revol_util:0.1-0.2'] = np.where((X['revol_util'] > 0.1) & (X['revol_util'] <= 0.2), 1, 0)
        X_new['revol_util:0.2-0.3'] = np.where((X['revol_util'] > 0.2) & (X['revol_util'] <= 0.3), 1, 0)
        X_new['revol_util:0.3-0.4'] = np.where((X['revol_util'] > 0.3) & (X['revol_util'] <= 0.4), 1, 0)
        X_new['revol_util:0.4-0.5'] = np.where((X['revol_util'] > 0.4) & (X['revol_util'] <= 0.5), 1, 0)
        X_new['revol_util:0.5-0.6'] = np.where((X['revol_util'] > 0.5) & (X['revol_util'] <= 0.6), 1, 0)
        X_new['revol_util:0.6-0.7'] = np.where((X['revol_util'] > 0.6) & (X['revol_util'] <= 0.7), 1, 0)
        X_new['revol_util:0.7-0.8'] = np.where((X['revol_util'] > 0.7) & (X['revol_util'] <= 0.8), 1, 0)
        X_new['revol_util:0.8-0.9'] = np.where((X['revol_util'] > 0.8) & (X['revol_util'] <= 0.9), 1, 0)
        X_new['revol_util:0.9-1.0'] = np.where((X['revol_util'] > 0.9) & (X['revol_util'] <= 1.0), 1, 0)
        X_new['revol_util:>1.0'] = np.where((X['revol_util'] > 1.0), 1, 0)
        X_new['out_prncp:<1,286'] = np.where((X['out_prncp'] <= 1286), 1, 0)
        X_new['out_prncp:1,286-6,432'] = np.where((X['out_prncp'] > 1286) & (X['out_prncp'] <= 6432), 1, 0)
        X_new['out_prncp:6,432-9,005'] = np.where((X['out_prncp'] > 6432) & (X['out_prncp'] <= 9005), 1, 0)
        X_new['out_prncp:9,005-10,291'] = np.where((X['out_prncp'] > 9005) & (X['out_prncp'] <= 10291), 1, 0)
        X_new['out_prncp:10,291-15,437'] = np.where((X['out_prncp'] > 10291) & (X['out_prncp'] <= 15437), 1, 0)
        X_new['out_prncp:>15,437'] = np.where((X['out_prncp'] > 15437), 1, 0)

        X_new['tot_cur_bal:0-100000'] = np.where((X['tot_cur_bal'] > 0) & (X['tot_cur_bal'] <= 100000), 1, 0)
        X_new['tot_cur_bal:100000-200000'] = np.where((X['tot_cur_bal'] > 100000) & (X['tot_cur_bal'] <= 200000), 1, 0)
        X_new['tot_cur_bal:200000-300000'] = np.where((X['tot_cur_bal'] > 200000) & (X['tot_cur_bal'] <= 300000), 1, 0)
        X_new['tot_cur_bal:300000-400000'] = np.where((X['tot_cur_bal'] > 300000) & (X['tot_cur_bal'] <= 400000), 1, 0)
        X_new['tot_cur_bal:400000-500000'] = np.where((X['tot_cur_bal'] > 400000) & (X['tot_cur_bal'] <= 500000), 1, 0)

        X_new.drop(columns = [], inplace = True)
        return X_new
# we could have also structured this class without the last drop statement and without creating categories out of the 
# feature categories. But doing the way we have done here allows us to keep a proper track of the categories, if required


to_prt = WoE_Binning(X)
to_prt_1= to_prt.transform(X_train.copy())

to_prt_1
```


### Ajuste del modelo

Para el ajuste del modelo se usará regresión logística cuyos parametros serán, y de acuerdo a un GridSearchCV realizado, max_iter=1000 y class_weight='auto'. A dicho modelo se la pasará un conjunto de datos que habrá pasado por la pipeline mencionada anteriormente:

```{python}
#| tbl-cap: Resumen del modelo entrenado
#| label: tbl-model-summary


import joblib
from sklearn.linear_model import LogisticRegression

# fit the pipeline on the whole training set
woe_transform = WoE_Binning(X)
reg = LogisticRegression(max_iter=1000, class_weight = 'auto')
pipeline = Pipeline(steps=[('woe', woe_transform), ('model', reg)])
pipeline.fit(X_train, y_train)


# first create a transformed training set through our WoE_Binning custom class
X_train_woe_transformed = woe_transform.fit_transform(X_train)
# Store the column names in X_train as a list
feature_name = X_train_woe_transformed.columns.values
# Create a summary table of our logistic regression model
summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)
# Create a new column in the dataframe, called 'Coefficients', with row values the transposed coefficients from the 'LogisticRegression' model
summary_table['Coefficients'] = np.transpose(pipeline['model'].coef_)
# Increase the index of every row of the dataframe with 1 to store our model intercept in 1st row
summary_table.index = summary_table.index + 1
# Assign our model intercept to this new row
summary_table.loc[0] = ['Intercept', pipeline['model'].intercept_[0]]
# Sort the dataframe by index
summary_table.sort_index(inplace = True)
summary_table
``` 

Ya con el modelo entrenado y con coeficientes para cada una de las variables como se puede ver en @tbl-model-summary se procederán a analizar resultados del ajuste: 

![Matriz de confusion](fig_m1.png){#fig-matriz-conf}

```{python}

# make preditions on our test set
y_hat_test = pipeline.predict(X_test)
# get the predicted probabilities
y_hat_test_proba = pipeline.predict_proba(X_test)

# make preditions on our test set
y_hat_test = pipeline.predict(X_test)
# get the predicted probabilities
y_hat_test_proba = pipeline.predict_proba(X_test)
# select the probabilities of only the positive class (class 1 - default) 
y_hat_test_proba = y_hat_test_proba[:][: , 1]

# create a temp y_test DF to reset its index to allow proper concaternation with y_hat_test_proba
y_test_temp = y_test.copy()
y_test_temp.reset_index(drop = True, inplace = True)
y_test_proba = pd.concat([y_test_temp, pd.DataFrame(y_hat_test_proba)], axis = 1)

# Rename the columns
y_test_proba.columns = ['y_test_class_actual', 'y_hat_test_proba']
# Makes the index of one dataframe equal to the index of another dataframe.
y_test_proba.index = X_test.index


# import seaborn as sns
# # assign a threshold value to differentiate good with bad
# tr = 0.5
# # crate a new column for the predicted class based on predicted probabilities and threshold
# # We will determine this optimat threshold later in this project
# y_test_proba['y_test_class_predicted'] = np.where(y_test_proba['y_hat_test_proba'] > tr, 1, 0)
# # create the confusion matrix
# cm1 = confusion_matrix(y_test_proba['y_test_class_actual'], y_test_proba['y_test_class_predicted'], normalize = 'all')
# sns.heatmap(cm1,annot=True,cbar=False)

```

Como se puede ver en @fig-matriz-conf el modelo ajustado es muy bueno para predecir verdaderos positivos, pero no para verdaderos negativos que los termina pasando como falsos positivos, lo que en contexto significaría otorgar créditos a muchas personas.

![Curva ROC](fig_m2.png){#fig-roc-curve-1}


En @fig-roc-curve-1 podemos observar el rendimiento de la curva de roc donde se nota que la convergencia a uno es relativamente rápida.  Por otra parte obtenemos un valor AUROC de 0.7751, es decir la probabilidad de que a una persona le aprueben un crédito y que en efecto cumple las caracteristicas para ser aprobado es del 77%.

### Creación de scorecard

Para comenzar, aquellos coeficientes obtenidos anteriormente se deben escalar con respecto a la escala escogida, en este caso vamos a basarnos en los puntajes establecidos por FICO que se encuentran en el intervalo 300-850. Una vez escalados los redondeamos para obtener el score preliminar.

Al calcular el máximo y el mínimo valor obtenido con el score preliminar (@tbl-scard-1) encontramos que el máximo es 851 que se pasa de el limite establecido inicialmente, para corregir esto sin afectar mucho el modelo inicial vamos a encontrar la máxima diferencia entre el score calculado y el preliminar entre los mayores valores de cada categoría, y a este le restamos uno. Este valor corresponde a inq_last_6mths:0 para el que el score calculado es 13.562276 y el score preliminar es 14, teniendo una diferencia de 0.43, este valor lo cambiaremos a 13 para que el máximo valor obtenido sea 850 en lugar de 851 sin afectar mucho a los datos preliminares, dicha corrección se puede ver en @tbl-scard-f.

```{python}
#| tbl-cap: Scorecard preliminar
#| label: tbl-scard-1


# We create a new dataframe with one column. Its values are the values from the 'reference_categories' list. We name it 'Feature name'.
df_ref_categories = pd.DataFrame(ref_categories, columns = ['Feature name'])
# We create a second column, called 'Coefficients', which contains only 0 values.
df_ref_categories['Coefficients'] = 0

# Concatenates two dataframes.
df_scorecard = pd.concat([summary_table, df_ref_categories])
# We reset the index of a dataframe.
df_scorecard.reset_index(inplace = True)

# create a new column, called 'Original feature name', which contains the value of the 'Feature name' column, up to the column symbol.
df_scorecard['Original feature name'] = df_scorecard['Feature name'].str.split(':').str[0]

# Define the min and max threshholds for our scorecard
min_score = 300
max_score = 850


# calculate the sum of the minimum coefficients of each category within the original feature name
min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].min().sum()
# calculate the sum of the maximum coefficients of each category within the original feature name
max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].max().sum()
# create a new columns that has the imputed calculated Score based on the multiplication of the coefficient by the ratio of the differences between
# maximum & minimum score and maximum & minimum sum of cefficients.
df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)
# update the calculated score of the Intercept (i.e. the default score for each loan)
df_scorecard.loc[0, 'Score - Calculation'] = ((df_scorecard.loc[0,'Coefficients'] - min_sum_coef) / (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score
# round the values of the 'Score - Calculation' column and store them in a new column
df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()
df_scorecard
```

Esta tabla tiene un uso muy básico, el cual se puede describir con la siguiente fórmula, en la que multiplicamos el valor de cada característica obtenida del usuario por el respectivo score final que encontramos en la tabla.

score = 598 + grade:A(54) + grade:B(32) + … + tot_cur_bal:400000-500000(13)

En este caso el intercepto corresponde al valor inicial para cada usuario antes de realizar el cálculo, donde dependiendo de los valores que tenga asociado a cada característica se suma o se resta para obtener un valor final de scored.

Para este problema podemos notar que el score máximo que se presenta es de 850, lo cual está en rango donde se tiene una buena vía crediticia, por otro lado tenemos que el mínimo es de 300, lo cual está en rango donde la vía crediticia es la peor.


```{python}
#| tbl-cap: Scorecard final
#| label: tbl-scard-f

df_scorecard['Difference'] = df_scorecard['Score - Preliminary'] - df_scorecard['Score - Calculation']

df_scorecard['Score - Final'] = df_scorecard['Score - Preliminary']
df_scorecard.loc[45, 'Score - Final'] = 13
df_scorecard
```

Teniendo en cuenta esto y calculando el scorecard para cada uno de los datos que se usaron para entrenar el modelo, obtenemos la distribución @fig-scard-distr.

![Analisis distribución dado el Scorecard](fig_2_scard_distr.png){#fig-scard-distr}

De @fig-scard-distr podemos notar que la media del scores es de 456, lo cual se encuentra en el rango más bajo de los puntajes crediticios, ya que está por debajo del quantil 25, además se observa que la distribución del score es bimodal ya que tiene dos picos, uno alrededor de 500 y el segundo alrededor de los 600.

### Variables más riesgosas

En la gráfica @fig-riesgo podemos observar el valor que aporta cada característica al scorcad final, teniendo en cuenta que el valor de referencia es el intercepto. En la gráfica superior podemos ver que la característica que más resta puntaje al cliente en su score es out_pmcp>1.286, seguida por los grados E y F, también se puede observar que la que menos le aporta negativamente es dti:15.196-19.195. En la gráfica inferior se tienen los diferentes pesos que aportan las características a los prestarios de manera positiva, por ejemplo el grado A es la que tiene un aporte de 54 aproximadamente, seguida por int_rate: < 7.071 y grado B, además se ve que la que tiene un menor aporte de manera positiva es reovol_ultiil:0.4-0.5.

![Analisis de variables más riesgoas](fig_3_riesgo.png){#fig-riesgo}


---
nocite: |
  @*
---

## Referencias

::: {#refs}
:::